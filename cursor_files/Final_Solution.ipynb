{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4108edb-f50a-4ea5-a0c5-04413116ccf5",
   "metadata": {},
   "source": [
    "### Mixture Density Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c23b6-119d-46f9-9fec-d547eb3d993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mdn import MixtureDensityNetwork, gmm_forward, get_argmax_mu, th2np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d823574",
   "metadata": {},
   "source": [
    "# --- 1. Parameters & Configuration ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fcc056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data 'training_data.npz'...\n",
      "Training data loaded successfully.\n",
      "Loaded 800 training samples.\n"
     ]
    }
   ],
   "source": [
    "# *** MODIFIED: This will now raise a FileNotFoundError if the file doesn't exist. ***\n",
    "print(\"Loading training data 'training_data.npz'...\")\n",
    "train_data = np.load('../data1/noisy_data.npz')\n",
    "x_train_np_orig = train_data['x']\n",
    "y_train_np_orig = train_data['y']\n",
    "print(\"Training data loaded successfully.\")\n",
    "\n",
    "N_SAMPLES = x_train_np_orig.shape[0]\n",
    "\n",
    "print(f\"Loaded {N_SAMPLES} training samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18662d9f",
   "metadata": {},
   "source": [
    "## --- 2. Data Preparation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cafc0b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for PyTorch\n",
    "if x_train_np_orig.ndim == 1:\n",
    "    x_train = x_train_np_orig.reshape(-1, 1)\n",
    "else:\n",
    "    x_train = x_train_np_orig\n",
    "\n",
    "if y_train_np_orig.ndim == 1:\n",
    "    y_train = y_train_np_orig.reshape(-1, 1)\n",
    "else:\n",
    "    y_train = y_train_np_orig\n",
    "    \n",
    "# Convert NumPy arrays to PyTorch Tensors\n",
    "x_train_tensor = torch.tensor(x_train).float()\n",
    "y_train_tensor = torch.tensor(y_train).float()\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce4517f",
   "metadata": {},
   "source": [
    "# --- 4. Model Init and Loss Function and Optimizer ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9df145c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MDN\n",
      "MixtureDensityNetwork(\n",
      "  (actv): Tanh()\n",
      "  (net): Sequential(\n",
      "    (linear_00): Linear(in_features=1, out_features=64, bias=True)\n",
      "    (tanh_01): Tanh()\n",
      "    (linear_02): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (tanh_03): Tanh()\n",
      "    (mixturesofgaussianlayer_04): MixturesOfGaussianLayer(\n",
      "      (fc_pi): Linear(in_features=64, out_features=4, bias=True)\n",
      "      (fc_mu): Linear(in_features=64, out_features=4, bias=True)\n",
      "      (fc_sigma): Linear(in_features=64, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing MDN\")\n",
    "model = MixtureDensityNetwork(\n",
    "    name='mdn_for_outliers',\n",
    "    x_dim=1,\n",
    "    y_dim=1,\n",
    "    k=4, \n",
    "    h_dim_list=[64, 64], # Mimics our old MLP structure\n",
    "    actv=nn.Tanh(),      # Use Tanh for a smoother function\n",
    "    p_drop=0.0,           # Turn off dropout for this simple problem\n",
    "    sig_max    = 3.0,\n",
    "    mu_min     = -2.5,\n",
    "    mu_max     = +2.5\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# --- 4. Optimizer ---\n",
    "\n",
    "# We DON'T define a 'criterion' (loss) here, it's calculated in the loop.\n",
    "# We can still use weight_decay (L2 regularization) on the base network\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f25045f",
   "metadata": {},
   "source": [
    "# --- 5. Training Loop ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b8577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting MDN training...\n",
      "Epoch [100/1000], NLL Loss: -0.0717\n",
      "Epoch [200/1000], NLL Loss: -0.1411\n",
      "Epoch [300/1000], NLL Loss: -0.2315\n",
      "Epoch [400/1000], NLL Loss: -0.2366\n",
      "Epoch [500/1000], NLL Loss: -0.2934\n",
      "Epoch [600/1000], NLL Loss: -0.3380\n",
      "Epoch [700/1000], NLL Loss: -0.3595\n",
      "Epoch [800/1000], NLL Loss: -0.3802\n",
      "Epoch [900/1000], NLL Loss: -0.2892\n",
      "Epoch [1000/1000], NLL Loss: -0.3541\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting MDN training...\")\n",
    "# Training Parameters\n",
    "NUM_EPOCHS = 1000\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        \n",
    "        # 1. Forward pass: Get the GMM parameters (pi, mu, sigma)\n",
    "        pi_batch, mu_batch, sigma_batch = model(x_batch)\n",
    "        \n",
    "        # 2. Compute Loss: Calculate the Negative Log-Likelihood\n",
    "        #    We use the 'gmm_forward' function we lifted\n",
    "        out_dict = gmm_forward(pi_batch, mu_batch, sigma_batch, y_batch)\n",
    "        nlls = out_dict['nlls'] # nlls is a tensor of shape [N]\n",
    "        loss = torch.mean(nlls) # Our loss is the average NLL\n",
    "        \n",
    "        # 3. Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Print progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], NLL Loss: {epoch_loss/len(train_loader):.4f}')\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9873b",
   "metadata": {},
   "source": [
    "# --- 6. Visualization with Validation Data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f714d687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data 'validation_data.npz'...\n",
      "Validation data loaded successfully.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'th2np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m     y_val_pred_tensor = get_argmax_mu(pi_val, mu_val)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Convert all data back to NumPy for plotting\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m y_val_pred_np = \u001b[43mth2np\u001b[49m(y_val_pred_tensor)\n\u001b[32m     27\u001b[39m x_val_np = th2np(x_val_tensor)\n\u001b[32m     28\u001b[39m y_val_np = y_val \u001b[38;5;66;03m# Already a numpy array\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'th2np' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Loading validation data 'validation_data.npz'...\")\n",
    "val_data = np.load('../data1/clean_data.npz')\n",
    "x_val = val_data['x']\n",
    "y_val = val_data['y']\n",
    "print(\"Validation data loaded successfully.\")\n",
    "    \n",
    "# Prepare validation data (reshape and convert to tensor)\n",
    "if x_val.ndim == 1: x_val = x_val.reshape(-1, 1)\n",
    "if y_val.ndim == 1: y_val = y_val.reshape(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val).float()\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get model predictions for the validation set\n",
    "with torch.no_grad():\n",
    "    # 1. Get the full GMM parameters\n",
    "    pi_val, mu_val, sigma_val = model(x_val_tensor)\n",
    "    \n",
    "    # 2. Get the *most probable* mean (our final prediction)\n",
    "    #    This is the key! We are no longer using a single, overfitted output.\n",
    "    y_val_pred_tensor = get_argmax_mu(pi_val, mu_val)\n",
    "\n",
    "# Convert all data back to NumPy for plotting\n",
    "y_val_pred_np = th2np(y_val_pred_tensor)\n",
    "x_val_np = th2np(x_val_tensor)\n",
    "y_val_np = y_val # Already a numpy array\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# Plot the original training data (with noise)\n",
    "plt.scatter(x_train_np_orig, y_train_np_orig, color='green', label='Noisy Training Data', s=10, alpha=0.2)\n",
    "\n",
    "# Plot the actual validation data (the \"ground truth\")\n",
    "plt.scatter(x_val_np, y_val_np, color='blue', label='Actual Validation Data', s=15, alpha=0.9)\n",
    "\n",
    "# Plot the model's predictions (The most probable mean)\n",
    "plt.scatter(x_val_np, y_val_pred_np, color='red', label='Model Predictions (Argmax Mu)', s=15, alpha=0.9)\n",
    "\n",
    "# Plot the prediction line\n",
    "sort_indices = np.argsort(x_val_np.squeeze())\n",
    "plt.plot(x_val_np[sort_indices], y_val_pred_np[sort_indices], color='red', linestyle='--', linewidth=2.5, label='Model Prediction Line')\n",
    "\n",
    "plt.title('MDN Prediction (Argmax Mu) vs. All Data')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.ylim(y_train_np_orig.min() - 0.5, y_train_np_orig.max() + 0.5) # Set Y-limits\n",
    "plt.xlim(x_train_np_orig.min() - 0.1, x_train_np_orig.max() + 0.1) # Set X-limits\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd9ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
