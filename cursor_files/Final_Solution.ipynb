{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4108edb-f50a-4ea5-a0c5-04413116ccf5",
   "metadata": {},
   "source": [
    "### Mixture Density Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1c23b6-119d-46f9-9fec-d547eb3d993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d823574",
   "metadata": {},
   "source": [
    "# --- 1. Parameters & Configuration ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8fcc056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 800 training samples.\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "# !!! UPDATE THIS FILENAME !!!\n",
    "try:\n",
    "    train_data = np.load('../data1/noisy_data.npz')\n",
    "    x_train = train_data['x']\n",
    "    y_train = train_data['y']\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'training_data.npz' not found.\")\n",
    "    print(\"Please create a dummy file or update the name to your training file.\")\n",
    "    # Create dummy data to allow the script to run for demonstration\n",
    "    x_train = np.linspace(-np.pi, np.pi, 800)\n",
    "    # True function (green line)\n",
    "    y_true_func = 0.5 * np.sin(x_train * 1.5) + 0.1 * x_train\n",
    "    # Add small noise\n",
    "    y_train = y_true_func + np.random.normal(0, 0.1, x_train.shape)\n",
    "    # Add outlier clusters\n",
    "    outlier_indices = np.random.choice(800, 100, replace=False)\n",
    "    y_train[outlier_indices] = y_train[outlier_indices] + np.random.uniform(-1.5, 1.5, 100)\n",
    "    print(\"Using dummy data for demonstration.\")\n",
    "\n",
    "# Get the total number of training samples\n",
    "N_SAMPLES = x_train.shape[0]\n",
    "\n",
    "# Set parameters based on the array size\n",
    "# We choose a power of 2 for batch size. 64 is efficient for 800 samples.\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3  # 0.001 is a good default for Adam\n",
    "NUM_EPOCHS = 1000     # Number of passes through the entire dataset\n",
    "\n",
    "print(f\"Loaded {N_SAMPLES} training samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18662d9f",
   "metadata": {},
   "source": [
    "## --- 2. Data Preparation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cafc0b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data: PyTorch models expect input shape (batch_size, num_features)\n",
    "# Our num_features is 1 (just the x-coordinate).\n",
    "# Original shape is likely (800,), we need (800, 1).\n",
    "if x_train.ndim == 1:\n",
    "    x_train = x_train.reshape(-1, 1)\n",
    "if y_train.ndim == 1:\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch Tensors\n",
    "# Use .float() as neural networks default to float32\n",
    "x_train_tensor = torch.tensor(x_train).float()\n",
    "y_train_tensor = torch.tensor(y_train).float()\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "# DataLoader handles batching and shuffling for us\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625e264",
   "metadata": {},
   "source": [
    "# --- 3. Model Definition ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3c1d3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegressionModel(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=32, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# We'll use a simple Multi-Layer Perceptron (MLP)\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(1, 32),    # Reduced to 32 neurons\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 32),   # Reduced to 32 neurons\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = RegressionModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987dde0e",
   "metadata": {},
   "source": [
    "# --- 4. Loss Function and Optimizer ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2fc5184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL CHOICE: Loss Function\n",
    "# We use HuberLoss (SmoothL1Loss) because it's robust to outliers.\n",
    "# Unlike MSE, it doesn't quadratically penalize large errors from outliers.\n",
    "criterion = nn.HuberLoss() \n",
    "# Alternative: nn.L1Loss() (Mean Absolute Error)\n",
    "# We AVOID: nn.MSELoss() (Mean Squared Error) which is very sensitive to outliers\n",
    "\n",
    "# Optimizer\n",
    "# Adam is a great general-purpose optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f25045f",
   "metadata": {},
   "source": [
    "# --- 5. Training Loop ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b8577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [100/1000], Loss: 0.1428\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Print progress every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {epoch_loss/len(train_loader):.4f}')\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c6fc98",
   "metadata": {},
   "source": [
    "# --- 6. Visualization with Validation Data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d6f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading validation data and plotting results...\")\n",
    "\n",
    "# Load the validation data\n",
    "# !!! UPDATE THIS FILENAME !!!\n",
    "try:\n",
    "    val_data = np.load('../data1/clean_data.npz')\n",
    "    x_val = val_data['x']\n",
    "    y_val = val_data['y']\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'validation_data.npz' not found. Using dummy validation data.\")\n",
    "    x_val = np.linspace(-np.pi, np.pi, 100)\n",
    "    y_val = 0.5 * np.sin(x_val * 1.5) + 0.1 * x_val # The \"clean\" green line\n",
    "    \n",
    "# Prepare validation data (reshape and convert to tensor)\n",
    "if x_val.ndim == 1:\n",
    "    x_val = x_val.reshape(-1, 1)\n",
    "if y_val.ndim == 1:\n",
    "    y_val = y_val.reshape(-1, 1)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val).float()\n",
    "\n",
    "# Set model to evaluation mode (e.g., turns off dropout)\n",
    "model.eval()\n",
    "\n",
    "# Get model predictions\n",
    "# We use torch.no_grad() to disable gradient calculation for efficiency\n",
    "with torch.no_grad():\n",
    "    y_val_pred_tensor = model(x_val_tensor)\n",
    "\n",
    "# Convert predictions and validation data back to NumPy for plotting\n",
    "y_val_pred = y_val_pred_tensor.numpy()\n",
    "x_val_np = x_val_tensor.numpy()\n",
    "y_val_np = y_val # Already a numpy array\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Plot the actual validation data (the \"ground truth\")\n",
    "plt.scatter(x_val_np, y_val_np, color='blue', label='Actual Validation Data', s=10, alpha=0.7)\n",
    "\n",
    "# Plot the model's predictions\n",
    "plt.scatter(x_val_np, y_val_pred, color='red', label='Model Predictions', s=10, alpha=0.7)\n",
    "\n",
    "# To better see the curve, we can sort the predicted values\n",
    "# (This is optional but makes the predicted line clearer)\n",
    "# sort_indices = np.argsort(x_val_np.squeeze())\n",
    "# plt.plot(x_val_np[sort_indices], y_val_pred[sort_indices], color='red', linestyle='--', linewidth=2, label='Model Prediction Line')\n",
    "\n",
    "\n",
    "plt.title('Model Predictions vs. Actual Validation Data')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
